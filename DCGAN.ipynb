{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/tejas/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import plot_model\n",
    "from PIL import Image\n",
    "from keras import Input\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras import Model\n",
    "from keras import losses\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "# MacOS matplotlib kernel issue\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Constants\n",
    "NOISE = (100,)\n",
    "IMAGE_SHAPE = (64,64,3)\n",
    "GAN_STEPS = 500\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(noise=NOISE):\n",
    "    gen_input = Input(shape=noise)\n",
    "    \n",
    "    generator = layers.Dense(8 * 8 * 256, use_bias=False)(gen_input)\n",
    "    generator = layers.LeakyReLU()(generator)\n",
    "    generator = layers.Reshape((8, 8, 256))(generator)\n",
    "    \n",
    "    generator = layers.UpSampling2D()(generator)\n",
    "    generator = layers.Conv2DTranspose(filters=256, kernel_size=(5,5), strides=(1,1), use_bias=False, padding='same', kernel_initializer='glorot_uniform')(generator)\n",
    "    generator = layers.BatchNormalization(momentum=0.5)(generator)\n",
    "    generator = layers.LeakyReLU()(generator)\n",
    "    \n",
    "    generator = layers.UpSampling2D()(generator)\n",
    "    generator = layers.Conv2DTranspose(filters=128, kernel_size=(4,4), strides=(1,1), use_bias=False, padding='same', kernel_initializer='glorot_uniform')(generator)\n",
    "    generator = layers.BatchNormalization(momentum=0.5)(generator)\n",
    "    generator = layers.LeakyReLU()(generator)\n",
    "    \n",
    "    generator = layers.UpSampling2D()(generator)\n",
    "    generator = layers.Conv2D(filters=64, kernel_size=(5,5), strides=(1,1), use_bias=False, padding='same', kernel_initializer='glorot_uniform')(generator)\n",
    "    generator = layers.BatchNormalization(momentum=0.5)(generator)\n",
    "    generator = layers.LeakyReLU()(generator)\n",
    "    \n",
    "    generator = layers.Conv2DTranspose(filters=64, kernel_size=(5,5), strides=(1,1), use_bias=False, padding='same', kernel_initializer='glorot_uniform')(generator)\n",
    "    generator = layers.BatchNormalization(momentum=0.5)(generator)\n",
    "    generator = layers.LeakyReLU()(generator)\n",
    "        \n",
    "    generator = layers.Conv2DTranspose(filters=3, kernel_size=(7,7), strides=(1,1), activation='tanh', use_bias=False, padding='same', kernel_initializer='glorot_uniform')(generator)\n",
    "    \n",
    "    model = Model(inputs=gen_input, outputs=generator)\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(image_shape=IMAGE_SHAPE):\n",
    "    disc_input = Input(shape=image_shape)\n",
    "    \n",
    "    discriminator = layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', strides=(1,1), kernel_initializer='glorot_uniform', kernel_regularizer=l2(1e-3))(disc_input)\n",
    "    discriminator = layers.LeakyReLU()(discriminator)\n",
    "    discriminator = layers.Dropout(0.3)(discriminator)\n",
    "    \n",
    "    discriminator = layers.Conv2D(filters=128, kernel_size=(4,4), padding='same', strides=(2,2), kernel_initializer='glorot_uniform', kernel_regularizer=l2(1e-3))(discriminator)\n",
    "    discriminator = layers.LeakyReLU()(discriminator)\n",
    "    discriminator = layers.Dropout(0.3)(discriminator)\n",
    "    \n",
    "    discriminator = layers.Conv2D(filters=256, kernel_size=(4,4), padding='same', strides=(2,2), kernel_initializer='glorot_uniform', kernel_regularizer=l2(1e-3))(discriminator)\n",
    "    discriminator = layers.LeakyReLU()(discriminator)\n",
    "    discriminator = layers.Dropout(0.3)(discriminator)\n",
    "    \n",
    "    discriminator = layers.Conv2D(filters=256, kernel_size=(4,4), padding='same', strides=(2,2), kernel_initializer='glorot_uniform', kernel_regularizer=l2(1e-3))(discriminator)\n",
    "    discriminator = layers.LeakyReLU()(discriminator)\n",
    "    discriminator = layers.Dropout(0.3)(discriminator)\n",
    "    \n",
    "    discriminator = layers.Flatten()(discriminator)\n",
    "#     discriminator = layers.Dropout(0.4)(discriminator)\n",
    "    discriminator = layers.Dense(1, activation='sigmoid')(discriminator)\n",
    "    \n",
    "    model = Model(inputs=disc_input, outputs=discriminator)\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16384)             1638400   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       1638400   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        102400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 64, 64, 3)         9408      \n",
      "=================================================================\n",
      "Total params: 4,119,744\n",
      "Trainable params: 4,118,720\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model = generator_model(NOISE)\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tejas/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 256)         1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16385     \n",
      "=================================================================\n",
      "Total params: 1,722,753\n",
      "Trainable params: 1,722,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model = discriminator_model()\n",
    "disc_model.summary()\n",
    "disc_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 64, 64, 3)         4119744   \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 1)                 1722753   \n",
      "=================================================================\n",
      "Total params: 5,842,497\n",
      "Trainable params: 4,118,720\n",
      "Non-trainable params: 1,723,777\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_gen_input = Input(shape=NOISE)\n",
    "gan_gen = gen_model(gan_gen_input)\n",
    "gan_dis = disc_model(gan_gen)\n",
    "\n",
    "gan_model = Model(inputs=gan_gen_input, outputs=gan_dis)\n",
    "gan_model.compile(optimizer=Adam(lr=1e-4), loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot_model(gen_model, to_file='./PlotsAndImages/gen_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(disc_model, to_file='./PlotsAndImages/disc_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "plot_model(gan_model, to_file='./PlotsAndImages/gan_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(predicted, current_time):\n",
    "    # Only 32 images will be printed\n",
    "    if BATCH_SIZE > 32:\n",
    "        predicted = predicted[:32]\n",
    "    num_images = predicted.shape[0]\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "    columns = 8\n",
    "    rows = np.ceil(num_images / columns)\n",
    "    for i in range(num_images):\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        my_image = predicted[i]\n",
    "        # Denormalize Image\n",
    "        my_image = ((my_image + 1) * 127.5) / 255\n",
    "#         my_image = (predicted[i] * 255).astype(np.uint8) / 255\n",
    "        plt.imshow(my_image)\n",
    "    plt.savefig('./GeneratedFigures/image_'+current_time+'.jpg', bbox_inches = 'tight', pad_inches = 0.1)\n",
    "#     plt.show(block=True)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL_get_images(batch_size=BATCH_SIZE, image_shape=IMAGE_SHAPE, data_search_pattern='./anime_face/000*'):\n",
    "    batch_image_shape = (batch_size,) + image_shape\n",
    "    batch_images = np.empty(batch_image_shape, dtype=np.float32)\n",
    "    all_images = glob.glob(data_search_pattern)\n",
    "    chosen_images = np.random.choice(all_images, batch_size)\n",
    "    for index, file in enumerate(chosen_images):\n",
    "        # Read images using PIL\n",
    "        my_img = Image.open(file).resize(IMAGE_SHAPE[:-1])\n",
    "        my_img = my_img.convert('RGB')\n",
    "        my_img = np.asarray(my_img, dtype=np.float)\n",
    "        my_img = (my_img / 127.5) - 1\n",
    "        batch_images[index,...] = my_img\n",
    "    return batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('./GeneratedFigures/*'):\n",
    "    if file.endswith('.jpg'):\n",
    "        os.remove(file)\n",
    "for file in glob.glob('./GANModels/*'):\n",
    "    if file.endswith('.h5'):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "               Step:  1\n",
      "\n",
      "**************************************\n",
      "Creating noise\n",
      "Predicting noise\n",
      "Training Real Discriminator\n",
      "disc_metrics\n",
      "[1.2429199, 0.234375]\n",
      "Training Fake Discriminator\n",
      "disc_metrics\n",
      "[1.2762804, 0.0]\n",
      "Creating GAN Noise\n",
      "Training GAN\n",
      "gan_metrics\n",
      "[0.86801803, 1.0]\n",
      "**************************************\n",
      "\n",
      "               Step:  2\n",
      "\n",
      "**************************************\n",
      "Creating noise\n",
      "Predicting noise\n",
      "Training Real Discriminator\n",
      "disc_metrics\n",
      "[0.7264592, 1.0]\n",
      "Training Fake Discriminator\n",
      "disc_metrics\n",
      "[5.475295, 0.0]\n",
      "Creating GAN Noise\n",
      "Training GAN\n",
      "gan_metrics\n",
      "[1.012995, 0.984375]\n",
      "**************************************\n",
      "\n",
      "               Step:  3\n",
      "\n",
      "**************************************\n",
      "Creating noise\n",
      "Predicting noise\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c0ea3fc1a6dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgen_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mNOISE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicting noise\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcreated_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msave_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreated_faces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('log.csv', 'w') as log:\n",
    "#     log.write('Step,DiscLoss,DiscAcc,GANLoss,GANAcc\\n')\n",
    "    log.write('Step,RealDiscLoss,RealDiscAcc,GenDiscLoss,GenDiscAcc,GANLoss,GANAcc\\n')\n",
    "for step in range(1, GAN_STEPS+1):\n",
    "    print('**************************************')\n",
    "    print()\n",
    "    print('               Step: ', step)\n",
    "    print()\n",
    "    print('**************************************')\n",
    "    \n",
    "    current_time = time.strftime('%Y_%m_%d_%H_%M_%S', time.localtime())\n",
    "    current_time = str(step)\n",
    "    print(\"Creating noise\")\n",
    "    gen_noise = np.random.normal(loc=0, scale=1, size=(BATCH_SIZE,)+NOISE)\n",
    "    print(\"Predicting noise\")\n",
    "    created_faces = gen_model.predict(gen_noise)\n",
    "    if ((step % 10) == 0):\n",
    "        save_fig(created_faces, current_time)\n",
    "    \n",
    "#     # Merge real and fake data\n",
    "#     real_faces = PIL_get_images()\n",
    "# #     print(\"Batch Index: \", train_gen.batch_index)\n",
    "#     combined_data = np.concatenate([real_faces, created_faces])\n",
    "#     combined_labels = np.concatenate([np.ones((BATCH_SIZE, 1), dtype=np.int).ravel(), np.zeros((BATCH_SIZE, 1), dtype=np.int).ravel()])\n",
    "    \n",
    "#     # Train Discriminator\n",
    "#     disc_model.trainable = True\n",
    "#     gen_model.trainable = False\n",
    "#     print(\"Training Discriminator\")\n",
    "#     disc_metrics = disc_model.train_on_batch(combined_data, combined_labels)\n",
    "#     print(\"disc_metrics\")\n",
    "#     print(disc_metrics)\n",
    "\n",
    "    # Merge real and fake data\n",
    "    real_faces = PIL_get_images()\n",
    "#     print(\"Batch Index: \", train_gen.batch_index)\n",
    "    real_labels = np.ones((BATCH_SIZE, 1), dtype=np.int).ravel()\n",
    "    fake_labels = np.zeros((BATCH_SIZE, 1), dtype=np.int).ravel()   \n",
    "    \n",
    "    # Train Discriminator\n",
    "    disc_model.trainable = True\n",
    "    gen_model.trainable = False\n",
    "    print(\"Training Real Discriminator\")\n",
    "    real_disc_metrics = disc_model.train_on_batch(real_faces, real_labels)\n",
    "    print(\"disc_metrics\")\n",
    "    print(real_disc_metrics)\n",
    "    print(\"Training Fake Discriminator\")\n",
    "    gen_disc_metrics = disc_model.train_on_batch(created_faces, fake_labels)\n",
    "    print(\"disc_metrics\")\n",
    "    print(gen_disc_metrics)\n",
    "\n",
    "\n",
    "    \n",
    "    # Train GAN\n",
    "    gen_model.trainable = True\n",
    "    disc_model.trainable = False\n",
    "    print(\"Creating GAN Noise\")\n",
    "    gan_noise = np.random.normal(loc=0, scale=1, size=(BATCH_SIZE,)+NOISE)\n",
    "    gan_labels = np.ones((BATCH_SIZE, 1), dtype=np.int).ravel()\n",
    "    print(\"Training GAN\")\n",
    "    gan_metrics = gan_model.train_on_batch(gan_noise, gan_labels)\n",
    "    print(\"gan_metrics\")\n",
    "    print(gan_metrics)\n",
    "    \n",
    "    # Append Log\n",
    "    with open('log.csv', 'a') as log:\n",
    "#         log.write('%d,%f,%f,%f,%f\\n' % (step, disc_metrics[0], disc_metrics[1], gan_metrics[0], gan_metrics[1]))\n",
    "        log.write('%d,%f,%f,%f,%f,%f,%f\\n' % (step, real_disc_metrics[0], real_disc_metrics[1], gen_disc_metrics[0], gen_disc_metrics[1], gan_metrics[0], gan_metrics[1]))\n",
    "    if ((step % 100) == 0):\n",
    "        gan_model.save('./GANModels/GANmodel_'+current_time+'.h5')\n",
    "        gen_model.trainable = True\n",
    "        gen_model.save('./GANModels/GENmodel_'+current_time+'.h5')\n",
    "        disc_model.trainable = True\n",
    "        disc_model.save('./GANModels/DISmodel_'+current_time+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
