{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate Results from StyleGAN2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEtpnr1HpyIy"
      },
      "source": [
        "# Generate Anime Faces using StyleGAN2\n",
        "This notebook makes use of my trained StyleGAN2 model that was trained using the dataset as mentioned in the [ReadMe.md](../ReadMe)   \n",
        "  \n",
        "There are three main files as described by StyleGAN2 to generate results: generate.py, style_mixing.py and projector.py.\n",
        "\n",
        "This is the PyTorch implementation of StyleGAN2 with ADA that provides faster results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGnr7RxRrRQY"
      },
      "source": [
        "Install PyTorch v1.7.1 and ninja. Ninja is needed to fix the issue `Setting up PyTorch plugin \"upfirdn2d_plugin\"... Failed!`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7jpXhDXS634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78b75ce9-7b98-445c-988d-3031e344f50d"
      },
      "source": [
        "!pip install --quiet torch==1.7.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install --quiet ninja\n",
        "import torch\n",
        "torch.__version__\n",
        "# Check Pytorch version 1.7.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.1+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctoclJ22r91o"
      },
      "source": [
        "Clone GitHub repository and set the directory for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDDPcmBKNkxz",
        "outputId": "c2f79596-0cd5-4abf-8237-06fcb08711f7"
      },
      "source": [
        "!git clone --quiet https://github.com/NVlabs/stylegan2-ada-pytorch\n",
        "%cd /content/stylegan2-ada-pytorch/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'stylegan2-ada-pytorch' already exists and is not an empty directory.\n",
            "/content/stylegan2-ada-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtAhIzWZO2HP",
        "outputId": "f6d66ffc-9e6a-4249-b3c7-9e430a650711"
      },
      "source": [
        "!python generate.py --help"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: generate.py [OPTIONS]\n",
            "\n",
            "  Generate images using pretrained network\n",
            "  pickle.\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  # Generate curated MetFaces images without truncation (Fig.10 left)\n",
            "  python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
            "\n",
            "  # Generate uncurated MetFaces images with truncation (Fig.12 upper left)\n",
            "  python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
            "\n",
            "  # Generate class conditional CIFAR-10 images (Fig.17 left, Car)\n",
            "  python generate.py --outdir=out --seeds=0-35 --class=1 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl\n",
            "\n",
            "  # Render an image from projected W\n",
            "  python generate.py --outdir=out --projected_w=projected_w.npz \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
            "\n",
            "Options:\n",
            "  --network TEXT                  Network pickle\n",
            "                                  filename\n",
            "                                  [required]\n",
            "\n",
            "  --seeds NUM_RANGE               List of random\n",
            "                                  seeds\n",
            "\n",
            "  --trunc FLOAT                   Truncation psi\n",
            "                                  [default: 1]\n",
            "\n",
            "  --class INTEGER                 Class label\n",
            "                                  (unconditional\n",
            "                                  if not\n",
            "                                  specified)\n",
            "\n",
            "  --noise-mode [const|random|none]\n",
            "                                  Noise mode\n",
            "                                  [default: const]\n",
            "\n",
            "  --projected-w FILE              Projection\n",
            "                                  result file\n",
            "\n",
            "  --outdir DIR                    Where to save\n",
            "                                  the output\n",
            "                                  images\n",
            "                                  [required]\n",
            "\n",
            "  --help                          Show this\n",
            "                                  message and\n",
            "                                  exit.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhaTuvlNO8IH",
        "outputId": "bbcaab5a-dce4-4e5e-ed91-8d5121156d6b"
      },
      "source": [
        "!rm -rf /content/results/generate\n",
        "network = '/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl'\n",
        "# seeds = '85,265,297,849'\n",
        "seeds = '100-105'\n",
        "trunc = 1\n",
        "noise_mode = 'const'\n",
        "outdir = '/content/results/generate'\n",
        "\n",
        "!python generate.py --outdir=$outdir --noise-mode=$noise_mode --trunc=$trunc --seeds=$seeds --network=$network"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl\"...\n",
            "Generating image for seed 100 (0/6) ...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "Generating image for seed 101 (1/6) ...\n",
            "Generating image for seed 102 (2/6) ...\n",
            "Generating image for seed 103 (3/6) ...\n",
            "Generating image for seed 104 (4/6) ...\n",
            "Generating image for seed 105 (5/6) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Oq54LRARpEh",
        "outputId": "7bb7c6f4-e369-4bd9-eea9-15ab7d306fdb"
      },
      "source": [
        "!python style_mixing.py --help"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: style_mixing.py [OPTIONS]\n",
            "\n",
            "  Generate images using pretrained network\n",
            "  pickle.\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  python style_mixing.py --outdir=out --rows=85,100,75,458,1500 --cols=55,821,1789,293 \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
            "\n",
            "Options:\n",
            "  --network TEXT                  Network pickle\n",
            "                                  filename\n",
            "                                  [required]\n",
            "\n",
            "  --rows NUM_RANGE                Random seeds to\n",
            "                                  use for image\n",
            "                                  rows  [required]\n",
            "\n",
            "  --cols NUM_RANGE                Random seeds to\n",
            "                                  use for image\n",
            "                                  columns\n",
            "                                  [required]\n",
            "\n",
            "  --styles NUM_RANGE              Style layer\n",
            "                                  range  [default:\n",
            "                                  0-6]\n",
            "\n",
            "  --trunc FLOAT                   Truncation psi\n",
            "                                  [default: 1]\n",
            "\n",
            "  --noise-mode [const|random|none]\n",
            "                                  Noise mode\n",
            "                                  [default: const]\n",
            "\n",
            "  --outdir TEXT                   [required]\n",
            "  --help                          Show this\n",
            "                                  message and\n",
            "                                  exit.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoceOJ5GTQeb",
        "outputId": "5c0af0b8-bbb3-4ece-cfb4-391754aed896"
      },
      "source": [
        "!rm -rf /content/results/style_mixing/\n",
        "network = '/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl'\n",
        "rows = '11,111,256,777,1212'\n",
        "cols = '32,1111,681,1074'\n",
        "styles = 6\n",
        "trunc = 1\n",
        "noise_mode = 'random'\n",
        "outdir = '/content/results/style_mixing'\n",
        "\n",
        "!python style_mixing.py --outdir=$outdir --rows=$rows --cols=$cols --styles=$styles --trunc=$trunc --noise-mode=$noise_mode \\\n",
        "        --network=$network"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl\"...\n",
            "Generating W vectors...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Generating images...\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "Generating style-mixed images...\n",
            "Saving images...\n",
            "Saving image grid...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn9q5bDdUnS9",
        "outputId": "2cbaa237-260a-4087-a42c-3369262f502c"
      },
      "source": [
        "!python projector.py --help"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage: projector.py [OPTIONS]\n",
            "\n",
            "  Project given image to the latent space of\n",
            "  pretrained network pickle.\n",
            "\n",
            "  Examples:\n",
            "\n",
            "  python projector.py --outdir=out --target=~/mytargetimg.png \\\n",
            "      --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
            "\n",
            "Options:\n",
            "  --network TEXT        Network pickle filename\n",
            "                        [required]\n",
            "\n",
            "  --target FILE         Target image file to\n",
            "                        project to  [required]\n",
            "\n",
            "  --num-steps INTEGER   Number of optimization\n",
            "                        steps  [default: 1000]\n",
            "\n",
            "  --seed INTEGER        Random seed  [default:\n",
            "                        303]\n",
            "\n",
            "  --save-video BOOLEAN  Save an mp4 video of\n",
            "                        optimization progress\n",
            "                        [default: True]\n",
            "\n",
            "  --outdir DIR          Where to save the output\n",
            "                        images  [required]\n",
            "\n",
            "  --help                Show this message and\n",
            "                        exit.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX5g5ThTVCUt",
        "outputId": "8661456b-ec8f-473e-fcdd-4daabafd71bf"
      },
      "source": [
        "!rm -rf /content/results/projector\n",
        "network = '/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl'\n",
        "target = '/content/target.png'\n",
        "num_steps = 100\n",
        "seed = 616\n",
        "save_video = True\n",
        "outdir = '/content/results/projector'\n",
        "\n",
        "!python projector.py --outdir=$outdir --target=$target --num-steps=$num_steps --seed=$seed --save-video=$save_video --network=$network"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl\"...\n",
            "Computing W midpoint and stddev using 10000 samples...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "step    1/100: dist 0.77 loss 10808.11\n",
            "step    2/100: dist 0.77 loss 8652.06\n",
            "step    3/100: dist 0.75 loss 6286.43\n",
            "step    4/100: dist 0.74 loss 3054.65\n",
            "step    5/100: dist 0.71 loss 1544.16\n",
            "step    6/100: dist 0.72 loss 2777.54\n",
            "step    7/100: dist 0.70 loss 6026.59\n",
            "step    8/100: dist 0.70 loss 6914.08\n",
            "step    9/100: dist 0.71 loss 6051.08\n",
            "step   10/100: dist 0.70 loss 4604.36\n",
            "step   11/100: dist 0.73 loss 3625.34\n",
            "step   12/100: dist 0.71 loss 2810.45\n",
            "step   13/100: dist 0.71 loss 2057.49\n",
            "step   14/100: dist 0.75 loss 1796.11\n",
            "step   15/100: dist 0.69 loss 1848.66\n",
            "step   16/100: dist 0.71 loss 1903.01\n",
            "step   17/100: dist 0.70 loss 1873.44\n",
            "step   18/100: dist 0.70 loss 1776.91\n",
            "step   19/100: dist 0.68 loss 1573.49\n",
            "step   20/100: dist 0.69 loss 1283.66\n",
            "step   21/100: dist 0.69 loss 978.44\n",
            "step   22/100: dist 0.69 loss 794.96\n",
            "step   23/100: dist 0.70 loss 792.49\n",
            "step   24/100: dist 0.69 loss 825.61\n",
            "step   25/100: dist 0.65 loss 809.75\n",
            "step   26/100: dist 0.66 loss 746.12\n",
            "step   27/100: dist 0.67 loss 665.59\n",
            "step   28/100: dist 0.66 loss 585.18\n",
            "step   29/100: dist 0.64 loss 506.06\n",
            "step   30/100: dist 0.66 loss 452.06\n",
            "step   31/100: dist 0.64 loss 397.49\n",
            "step   32/100: dist 0.63 loss 340.05\n",
            "step   33/100: dist 0.63 loss 285.13\n",
            "step   34/100: dist 0.63 loss 246.85\n",
            "step   35/100: dist 0.64 loss 231.89\n",
            "step   36/100: dist 0.64 loss 223.01\n",
            "step   37/100: dist 0.64 loss 224.38\n",
            "step   38/100: dist 0.64 loss 229.73\n",
            "step   39/100: dist 0.63 loss 224.19\n",
            "step   40/100: dist 0.64 loss 196.15\n",
            "step   41/100: dist 0.64 loss 163.02\n",
            "step   42/100: dist 0.63 loss 140.73\n",
            "step   43/100: dist 0.61 loss 123.63\n",
            "step   44/100: dist 0.62 loss 103.33\n",
            "step   45/100: dist 0.61 loss 88.85\n",
            "step   46/100: dist 0.61 loss 80.71\n",
            "step   47/100: dist 0.61 loss 72.89\n",
            "step   48/100: dist 0.61 loss 68.71\n",
            "step   49/100: dist 0.61 loss 66.32\n",
            "step   50/100: dist 0.61 loss 69.80\n",
            "step   51/100: dist 0.60 loss 63.83\n",
            "step   52/100: dist 0.60 loss 51.18\n",
            "step   53/100: dist 0.60 loss 43.28\n",
            "step   54/100: dist 0.58 loss 35.83\n",
            "step   55/100: dist 0.58 loss 29.01\n",
            "step   56/100: dist 0.58 loss 26.52\n",
            "step   57/100: dist 0.57 loss 28.55\n",
            "step   58/100: dist 0.57 loss 32.05\n",
            "step   59/100: dist 0.57 loss 30.10\n",
            "step   60/100: dist 0.57 loss 28.81\n",
            "step   61/100: dist 0.57 loss 27.54\n",
            "step   62/100: dist 0.57 loss 23.64\n",
            "step   63/100: dist 0.56 loss 17.34\n",
            "step   64/100: dist 0.56 loss 11.35\n",
            "step   65/100: dist 0.56 loss 8.34 \n",
            "step   66/100: dist 0.56 loss 8.35 \n",
            "step   67/100: dist 0.56 loss 10.87\n",
            "step   68/100: dist 0.55 loss 13.17\n",
            "step   69/100: dist 0.55 loss 12.90\n",
            "step   70/100: dist 0.55 loss 10.73\n",
            "step   71/100: dist 0.55 loss 8.10 \n",
            "step   72/100: dist 0.55 loss 5.35 \n",
            "step   73/100: dist 0.56 loss 3.76 \n",
            "step   74/100: dist 0.55 loss 3.82 \n",
            "step   75/100: dist 0.55 loss 5.08 \n",
            "step   76/100: dist 0.55 loss 6.86 \n",
            "step   77/100: dist 0.55 loss 8.42 \n",
            "step   78/100: dist 0.54 loss 9.69 \n",
            "step   79/100: dist 0.54 loss 9.32 \n",
            "step   80/100: dist 0.54 loss 6.33 \n",
            "step   81/100: dist 0.54 loss 2.27 \n",
            "step   82/100: dist 0.54 loss 1.91 \n",
            "step   83/100: dist 0.54 loss 3.54 \n",
            "step   84/100: dist 0.53 loss 2.78 \n",
            "step   85/100: dist 0.53 loss 3.12 \n",
            "step   86/100: dist 0.53 loss 3.46 \n",
            "step   87/100: dist 0.53 loss 2.31 \n",
            "step   88/100: dist 0.53 loss 2.45 \n",
            "step   89/100: dist 0.53 loss 1.40 \n",
            "step   90/100: dist 0.53 loss 1.38 \n",
            "step   91/100: dist 0.53 loss 0.92 \n",
            "step   92/100: dist 0.53 loss 0.80 \n",
            "step   93/100: dist 0.53 loss 0.91 \n",
            "step   94/100: dist 0.53 loss 0.75 \n",
            "step   95/100: dist 0.53 loss 0.71 \n",
            "step   96/100: dist 0.53 loss 0.76 \n",
            "step   97/100: dist 0.53 loss 0.76 \n",
            "step   98/100: dist 0.53 loss 0.73 \n",
            "step   99/100: dist 0.53 loss 0.71 \n",
            "step  100/100: dist 0.53 loss 0.69 \n",
            "Elapsed: 13.5 s\n",
            "Saving optimization progress video \"/content/results/projector/proj.mp4\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoigSN3YZA0R",
        "outputId": "43fd74d4-64a9-41c3-bc2e-e97bf6a529fa"
      },
      "source": [
        "!rm -rf /content/results/generate_projected\n",
        "network = '/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl'\n",
        "# seeds = '85,265,297,849'\n",
        "# trunc = 1\n",
        "# noise_mode = 'const'\n",
        "projected_w = '/content/results/projector/projected_w.npz'\n",
        "outdir = '/content/results/generate_projected'\n",
        "\n",
        "!python generate.py --outdir=$outdir --projected-w=$projected_w --network=$network"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading networks from \"/content/drive/MyDrive/StyleGAN2/network-snapshot-000880.pkl\"...\n",
            "Generating images from projected W \"/content/results/projector/projected_w.npz\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}